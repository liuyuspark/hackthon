{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK0cINhn5as5",
        "outputId": "2c7d6155-687a-4ba7-a1d0-cf59fef4e154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.14.2-py3-none-any.whl (262 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.4/262.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.14.2\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "# with open(\"drive/MyDrive/Key.txt\",\"r\") as f:\n",
        "#     openai.api_key = f.read()\n",
        "openai.api_key = \"a4dd22698d4b4b25bbf323ee7bcec472\""
      ],
      "metadata": {
        "id": "kusNyUDP65IG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"empty\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7__qZhck_vBV",
        "outputId": "bd2a2f0b-4b4d-4305-d1c7-9e5d5a30643e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at empty\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n",
        "!pip install numpy\n",
        "!pip install collection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NN9RfyEA9Ko",
        "outputId": "96bedfa5-acce-483e-f2e9-a62d5100b717"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.8 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.8 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.8 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting collection\n",
            "  Downloading collection-0.1.6.tar.gz (5.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: collection\n",
            "  Building wheel for collection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for collection: filename=collection-0.1.6-py3-none-any.whl size=5099 sha256=b0d195eef91842f639abbe6f6bad5de382f1f844fa6cfe703cb24b5f8b0476ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/70/eb/1d28795e9384ab3b9be6359bdde9e1652f6e7dab9d26844f70\n",
            "Successfully built collection\n",
            "Installing collected packages: collection\n",
            "Successfully installed collection-0.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S26uqRcdKgNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "OXb1RHJxBrS3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"empty/MyDrive/Colab Notebooks/huohua_data_0308/CategoryOnly/train_sop905_0207.jsonl\"\n"
      ],
      "metadata": {
        "id": "Q7qoy209BrOW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(data_path) as f:\n",
        "  json_dataset = [json.loads(line) for line in f]"
      ],
      "metadata": {
        "id": "fyzKedX0PCnf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_dataset[0]"
      ],
      "metadata": {
        "id": "r9nv-DWeBmIC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac9f2541-e89f-4fbb-a755-979a8afaeb3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [{'role': 'system',\n",
              "   'content': '\\n你是一个资深的内容质量检测专家，擅长检测文本中是否具有关于“补差”的关键信息。“补差”是用户在完成第一节课后的30天内，有资格以比第一次购买课包更优惠的价格升级到大课包，补中间的差价获得相应的课时和权益。\\n\\n你的任务\\n对提供的文字内容进行检测，判断该内容中是否含有关于课程升级补差的关键信息点，据此进行分类并按要求输出结果。\\n\\n你需要检测3个关键信息点：\\n1. 补差权益：文字中提到补差【3900】元就可以获得【48】节直播课时，【4】次请假机会，【2】节互动自学课，【1】次休学机会，【30】次周分享机会（可兑换6节直播课课时）等含义相近的关于价格优惠和购买内容的具体信息，例如，便宜了2000/800块钱。\\n2. 权益到期：文字中提到补差优惠的有效期限、付款截止日期、权益失效的时间等，注意二维码过期不代表补差权益到期，例如，今天/还有几天就过期了，今天是补差最后一天了，马上就到期/过期/失效了，还有XX个小时就截止了，截止到23:59。\\n3. 已经补差：表示用户已经付款、课时和权益已经充值到用户账户的描述，注意下了订单不代表已经付款、支付成功跟我说/截图给我不代表已经付款，例如，课时已经到账了、已经登记上了、订单发票、抽奖链接：https://m.huohua.cn/activity/...。\\n\\n你的思考过程\\n先找到文字中的关键信息，再进行分类。\\n\\n你的输出\\n如果文字中没有包含上面任何1个信息，请输出0；\\n如果文字中只包含<补差权益>或者<权益到期>其中1个信息，请输出1；\\n如果文字中包含全部<补差权益>和<权益到期>的2个信息，请输出2；\\n如果文字中包含<已经补差>的信息，请输出2。\\n\\n注意：只输出1个数字，不需要其他说明。\\n注意：只输出1个数字，不需要其他说明。\\n注意：只输出1个数字，不需要其他说明。\\n'},\n",
              "  {'role': 'user',\n",
              "   'content': '酒窝妈妈  咱们补差权益今天过期  宝贝目前状态也比较稳定  能够说以一个较低的价格去往后规划半年的课程性价比还是很高的这个补差权益确实是新生专属的权益  过了就没有了也是不希望咱们错过其实退一步讲 哪怕后续计划有变 咱们也是随学随退的哈这一点是可以保证的我这边给咱们生成链接哈这是补差订单  微信和支付宝都可以  您支付成功后辛苦跟我说一下  我给咱们核对课时和权益~11月补差还额外享有4套轻课和一次百分百抽奖机会哟~妈妈咱们课时到账了哈~辛苦核对~抽奖链接：https://m.huohua.cn/activity/april/globalLuckDraw/HUOHUA_Nov1_2023这是咱们的抽奖链接  妈妈点进去抽奖哦   实物礼品会寄送哒感谢妈妈的信任和支持[爱心][爱心]目前L3的课程考虑到孩子的专注度和接受度上是30分钟 \\n等这个级别结束后  咱们升级别到L4 就是差不多明天8月份   就是50分钟了 上20 休10 再上20 到L7的话 就是一个小时  25+10+25 如果说后期时间紧张 孩子接受度高的话我们也有一周一次课  一次上两节的排课嗯嗯 有啥问题随时喊我就成'},\n",
              "  {'role': 'assistant', 'content': '2'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "\n",
        "# 加载JSON数据集\n",
        "data_path = \"empty/MyDrive/Colab Notebooks/huohua_data_0308/CategoryOnly/train_sop905_0207.jsonl\"\n",
        "with open(data_path) as f:\n",
        "  json_dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# 设置您的OpenAI API密钥\n",
        "openai.api_key = \"a4dd22698d4b4b25bbf323ee7bcec472\"\n",
        "\n",
        "# 定义处理单个数据的函数\n",
        "def process_data(data):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=data[\"messages\"]\n",
        "    )\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "# 循环处理每个数据\n",
        "results = []\n",
        "for data in json_dataset:\n",
        "    result = process_data(data)\n",
        "    results.append(result)\n",
        "\n",
        "# 输出结果\n",
        "for i, result in enumerate(results):\n",
        "    print(f\"Data {i}: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "vIwMpdFXW8-a",
        "outputId": "ffdd8485-0c5e-4170-a698-a8209812b0b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "APIRemovedInV1",
          "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-b383cd0a545d>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-b383cd0a545d>\u001b[0m in \u001b[0;36mprocess_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 定义处理单个数据的函数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/lib/_old_api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAPIRemovedInV1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNtwiE0JY0qD",
        "outputId": "c56e24e9-dac0-4cf6-954d-bfb88a80a266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: openai 1.14.2\n",
            "Uninstalling openai-1.14.2:\n",
            "  Would remove:\n",
            "    /usr/local/bin/openai\n",
            "    /usr/local/lib/python3.10/dist-packages/openai-1.14.2.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/openai/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled openai-1.14.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uo9R8b3hY3-4",
        "outputId": "8a82948e-f16b-4220-9f0c-16067ad122d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Using cached openai-1.14.2-py3-none-any.whl (262 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-1.14.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICw3L830aDsv",
        "outputId": "c2c7664d-c8d9-4ec9-cc56-e9a1a52546da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.14.2-py3-none-any.whl (262 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/262.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m225.3/262.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.4/262.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.14.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"empty/MyDrive/Colab Notebooks/huohua_data_0308/CategoryOnly/train_sop905_0207.jsonl\"\n"
      ],
      "metadata": {
        "id": "R_VvzB191Yk3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from openai import AzureOpenAI\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# 设置环境变量\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"a4dd22698d4b4b25bbf323ee7bcec472\"\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://tp-east-us.openai.azure.com/\"\n",
        "\n",
        "# 加载JSON数据集\n",
        "data_path = \"/content/drive/MyDrive/Colab Notebooks/huohua_data_0308/CategoryOnly/test_sop905_0207.jsonl\"\n",
        "with open(data_path) as f:\n",
        "    json_dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# 设置 Azure OpenAI 客户端\n",
        "client = AzureOpenAI(\n",
        "    api_version=\"2023-12-01-preview\",\n",
        "    azure_endpoint=os.getenv(\"a4dd22698d4b4b25bbf323ee7bcec472\")\n",
        ")\n",
        "\n",
        "# 定义处理单个数据的函数\n",
        "def process_data(data):\n",
        "    messages = data[\"messages\"]\n",
        "    total_tokens = sum(len(msg[\"content\"].split()) for msg in messages)\n",
        "    max_tokens = 4000  # 设置一个略低于4096的上限\n",
        "    if total_tokens > max_tokens:\n",
        "        # 如果总token数超过上限,截断messages\n",
        "        new_messages = []\n",
        "        current_tokens = 0\n",
        "        for msg in messages:\n",
        "            msg_tokens = len(msg[\"content\"].split())\n",
        "            if current_tokens + msg_tokens > max_tokens:\n",
        "                break\n",
        "            new_messages.append(msg)\n",
        "            current_tokens += msg_tokens\n",
        "        messages = new_messages\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        model=\"AIInnov-Jinse-GPT35-turbo-2\",\n",
        "        messages=messages\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content\n",
        "\n",
        "# 循环处理每个数据\n",
        "results = []\n",
        "for data in json_dataset:\n",
        "    result = process_data(data)\n",
        "    results.append(result)\n",
        "\n",
        "# 输出结果\n",
        "for i, result in enumerate(results):\n",
        "    print(f\"Data {i}: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "hKDxw2SDYkEU",
        "outputId": "f85c2d15-d387-4f9f-82ad-6a7c1cdd3881"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 4096 tokens. However, your messages resulted in 6458 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-bde830ba9fe5>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-bde830ba9fe5>\u001b[0m in \u001b[0;36mprocess_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mcurrent_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmsg_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     chat_completion = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"AIInnov-Jinse-GPT35-turbo-2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 667\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    668\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         )\n\u001b[0;32m-> 1208\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 897\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    898\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4096 tokens. However, your messages resulted in 6458 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import codecs  # Import codecs module\n",
        "from openai import AzureOpenAI\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"a4dd22698d4b4b25bbf323ee7bcec472\"\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://tp-east-us.openai.azure.com/\"\n",
        "\n",
        "# Load JSON dataset\n",
        "data_path = \"/content/drive/MyDrive/Colab Notebooks/huohua_data_0308/CategoryOnly/valid_sop905_0207.jsonl\"\n",
        "\n",
        "# Use `codecs.open()` to handle non-breaking spaces\n",
        "with codecs.open(data_path, encoding='utf-8-sig') as f:\n",
        "    json_dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# Set Azure OpenAI client\n",
        "client = AzureOpenAI(\n",
        "    api_version=\"2023-12-01-preview\",\n",
        "    azure_endpoint=os.getenv(\"a4dd22698d4b4b25bbf323ee7bcec472\")\n",
        ")\n",
        "\n",
        "# Define function to process a single data item\n",
        "def process_data(data):\n",
        "    messages = data[\"messages\"]\n",
        "    total_tokens = sum(len(msg[\"content\"].split()) for msg in messages)\n",
        "    max_tokens = 4000  # Set a slightly lower limit than 4096\n",
        "\n",
        "    # Check if conversation is too long\n",
        "    if total_tokens > max_tokens:\n",
        "        print(f\"Conversation {data.get('id', 'NA')} is too long, skipping...\")\n",
        "        return None\n",
        "\n",
        "    # Truncate conversation (optional)\n",
        "    if total_tokens > 3500:  # Adjust threshold based on your data\n",
        "        new_messages = []\n",
        "        current_tokens = 0\n",
        "        for msg in messages:\n",
        "            msg_tokens = len(msg[\"content\"].split())\n",
        "            if current_tokens + msg_tokens > max_tokens:\n",
        "                break\n",
        "            new_messages.append(msg)\n",
        "            current_tokens += msg_tokens\n",
        "        messages = new_messages\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        model=\"AIInnov-Jinse-GPT35-turbo-2\",\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    # Check if there are any choices in the response\n",
        "    if chat_completion.choices:\n",
        "        try:\n",
        "            # Extract the predicted category and confidence score\n",
        "            category = chat_completion.choices[0].message.content.split(\":\")[0]\n",
        "            confidence = float(chat_completion.choices[0].message.content.split(\":\")[1])\n",
        "        except Exception as e:\n",
        "            # Handle errors during category extraction\n",
        "            print(f\"Error extracting category for data {data.get('id', 'NA')}: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"No choices returned for data {data.get('id', 'NA')}\")\n",
        "        return None\n",
        "\n",
        "    return {\"category\": category, \"confidence\": confidence}\n",
        "\n",
        "# Process each data item\n",
        "results = []\n",
        "skipped_count = 0\n",
        "for data in json_dataset:\n",
        "    try:\n",
        "        result = process_data(data)\n",
        "        if result is not None:\n",
        "            results.append(result)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing data {data.get('id', 'NA')}: {e}\")\n",
        "        skipped_count += 1\n",
        "\n",
        "# Output results\n",
        "print(f\"Processed {len(results)} conversations, skipped {skipped_count} due to length, errors, or no choices.\")\n",
        "for i, result in enumerate(results):\n",
        "    print(f\"Data {i}: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqEKTbal4udP",
        "outputId": "defae2ed-5f81-4703-b0cb-7ba06da04418"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error processing data NA: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4096 tokens. However, your messages resulted in 6900 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error processing data NA: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4096 tokens. However, your messages resulted in 5968 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Error extracting category for data NA: list index out of range\n",
            "Processed 0 conversations, skipped 2 due to length, errors, or no choices.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client35 = AzureOpenAI(\n",
        "    api_key=\"a4dd22698d4b4b25bbf323ee7bcec472\",\n",
        "    api_version=\"2023-12-01-preview\",\n",
        "    azure_endpoint=\"https://tp-east-us.openai.azure.com/\",\n",
        ")\n",
        "client35.chat.completions.create(\n",
        "    model=\"AIInnov-Jinse-GPT35-turbo-2\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\\n你是一个资深的内容质量检测专家，擅长检测文本中是否具有关于“补差”的关键信息。“补差”是用户在完成第一节课后的30天内，有资格以比第一次购买课包更优惠的价格升级到大课包，补中间的差价获得相应的课时和权益。\\n\\n你的任务\\n对提供的文字内容进行检测，判断该内容中是否含有关于课程升级补差的关键信息点，据此进行分类并按要求输出结果。\\n\\n你需要检测3个关键信息点：\\n1. 补差权益：文字中提到补差【3900】元就可以获得【48】节直播课时，【4】次请假机会，【2】节互动自学课，【1】次休学机会，【30】次周分享机会（可兑换6节直播课课时）等含义相近的关于价格优惠和购买内容的具体信息，例如，便宜了2000/800块钱。\\n2. 权益到期：文字中提到补差优惠的有效期限、付款截止日期、权益失效的时间等，注意二维码过期不代表补差权益到期，例如，今天/还有几天就过期了，今天是补差最后一天了，马上就到期/过期/失效了，还有XX个小时就截止了，截止到23:59。\\n3. 已经补差：表示用户已经付款、课时和权益已经充值到用户账户的描述，注意下了订单不代表已经付款、支付成功跟我说/截图给我不代表已经付款，例如，课时已经到账了、已经登记上了、订单发票、抽奖链接：https://m.huohua.cn/activity/...。\\n\\n你的思考过程\\n先找到文字中的关键信息，再进行分类。\\n\\n你的输出\\n如果文字中没有包含上面任何1个信息，请输出0；\\n如果文字中只包含<补差权益>或者<权益到期>其中1个信息，请输出1；\\n如果文字中包含全部<补差权益>和<权益到期>的2个信息，请输出2；\\n如果文字中包含<已经补差>的信息，请输出2。\\n\\n注意：只输出1个数字，不需要其他说明。\\n注意：只输出1个数字，不需要其他说明。\\n注意：只输出1个数字，不需要其他说明。\\n\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": 'L4课程大纲： https://m.huohua.cn/huoziv3/page/vlObgQckwmDlQJUz?lang=zh-CN 糯糯妈妈您看看这个是咱们L4课程大纲、主要核心知识点，还有培养孩子主要学习的5类题型需要运用的方法， 对应孩子小一的重要对标知识举例这是一条引用/回复消息：\\n\"🍓殷小喵neko🍑：\\n然后拼音的课程是不是有的啊\"\\n------\\n这个我给咱们请中文老师给咱们安排下试听 您看好嘛 ~ 好滴好滴~ 可以拍小视频发给我 哈哈下午好，提醒糯糯今天在火花有课哦，我们按时上课~ \\n课次👉：《机位拍照》，上课时间👉：2023-10-19 19:15:00 \\n\\n【火花秘籍】【小老师视频】今天课后咱们及时完成，在家长端app上传一下小老师视频👌糯糯妈妈您好~ 昨天和您提醒的3900元  48课时的补差权益，咱们可以看看哦~ 补差权益只剩今天1天，过期之后不能再补了[苦涩]~\\n\\n此时补差的优点罗列了几条，您看下哈\\n1、课时单价低\\n2、咱们的课程是全年制的，共96节课，目前咱们的课时无法满足宝贝全年系统的学习\\n3、如后期什么原因导致不能继续上课，咱们这个是可以按课时退的，所以建议咱们先补上，保留权益\\n同时综合考虑到您为孩子学习投资的性价比问题，以及涉及到孩子的一个长期学习规划，所以我想跟您沟通得更清楚些哟[抱拳]对的 第一个是48课时+10节赠课（通过伴读奖+周分享等）第二个补差权益是48课时+6节赠课（30次周分享打卡，都会提醒我们的 一周一次）糯糯妈妈咱们考虑的咋样啦 ~我从后台下单  发给咱们 ~ 有的 ~ 我这边给咱们送一次百分百中奖的抽奖次数还有拓展课~是这个~ 拓展课的内容比较多， 有《10节益智故事学思维》，《5节益智动手桌游课，《12节数独趣味课》，《5节汽车华容道一一逆向思维策略游戏》在咱们家长端， 学生端， 都可以带孩子学习这个也是咱们专门研发的课程， 孩子在学生端， 我的课程，拓展课就能看啦 咱们百分百中奖的抽奖机会，至少会抽到一个实物礼品的，这个也特别好滴 而且主要补差权益是课单价特别优惠，只有新生有哒对的我给咱们发过来补差订单报名码哦 这是一条引用/回复消息：\\n\"🍓殷小喵neko🍑：\\n是先用我现在的课程，到时候没用这个是可以随时全额退款的吧\"\\n------\\n对的先用我们第一个课包， 课时是先进先出的，咱们上一节课消耗一节课时后面的补差课包未开启是能全额退款的 ， 开启后也是按照剩余的课时比例退款，不会额外扣除啥赠课现在您可以先兑换下， 就会在补差课包之前消耗啦我们现在有20000多火花币你兑换之后， 就可以保存这个报名码 用微信支付宝都可以的， 报名码是10分钟有效，您付了跟我说下， 我帮咱们核对课时~ 就是这个 没有问题~兑换的没有问题~咱们每周继续参与周分享打卡~ 还有第二个伴读奖的就能获得啦好嘞 咱们课时等都到啦 ，看咱们要开购课发票吗~ 发票内容是这个 三选一哦 可以的 个人的公司的都行是电子发票好滴好滴 您问一下 上次的也可以一块开的分开开两个',\n",
        "        },\n",
        "    ],\n",
        ").choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2n2WieWwE7oL",
        "outputId": "693bce64-3c6e-488b-86b1-70331bde4348"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import codecs\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"a4dd22698d4b4b25bbf323ee7bcec472\"\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://tp-east-us.openai.azure.com/\"\n",
        "\n",
        "# Load JSON dataset\n",
        "data_path = \"/content/drive/MyDrive/Colab Notebooks/huohua_data_0308/CategoryOnly/valid_sop905_0207.jsonl\"\n",
        "with codecs.open(data_path, encoding='utf-8-sig') as f:\n",
        "    json_dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# Set Azure OpenAI client\n",
        "client35 = AzureOpenAI(\n",
        "    api_key=os.getenv(\"a4dd22698d4b4b25bbf323ee7bcec472\"),\n",
        "    api_version=\"2023-12-01-preview\",\n",
        "    azure_endpoint=os.getenv(\"\"https://tp-east-us.openai.azure.com/\")\n",
        ")\n",
        "\n",
        "# Process the first data item\n",
        "data = json_dataset[0]\n",
        "messages = data[\"messages\"]\n",
        "\n",
        "# Create the completion\n",
        "completion = client35.chat.completions.create(\n",
        "    model=\"AIInnov-Jinse-GPT35-turbo-2\",\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "# Extract the predicted category and confidence score\n",
        "category = completion.choices[0].message.content.split(\":\")[0]\n",
        "confidence = float(completion.choices[0].message.content.split(\":\")[1])\n",
        "\n",
        "# Output result\n",
        "print(f\"Processed data: {{'category': {category}, 'confidence': {confidence}}}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "ZbkZ-mPbF3tv",
        "outputId": "f35b3850-a3d3-451e-a903-fa740a46e664"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 19) (<ipython-input-31-4cbacc7b5be3>, line 19)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-4cbacc7b5be3>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    azure_endpoint=os.getenv(\"\"https://tp-east-us.openai.azure.com/\")\u001b[0m\n\u001b[0m                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import codecs\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"a4dd22698d4b4b25bbf323ee7bcec472\"\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://tp-east-us.openai.azure.com/\"\n",
        "\n",
        "# Load JSON dataset\n",
        "data_path = \"/content/drive/MyDrive/Colab Notebooks/huohua_data_0308/CategoryOnly/valid_sop905_0207.jsonl\"\n",
        "with codecs.open(data_path, encoding='utf-8-sig') as f:\n",
        "    json_dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# Set Azure OpenAI client\n",
        "client35 = AzureOpenAI(\n",
        "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
        "    api_version=\"2023-12-01-preview\",\n",
        "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        ")\n",
        "\n",
        "# Process the first data item\n",
        "data = json_dataset[0]\n",
        "messages = data[\"messages\"]\n",
        "\n",
        "# Create the completion\n",
        "completion = client35.chat.completions.create(\n",
        "    model=\"AIInnov-Jinse-GPT35-turbo-2\",\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "# Check if there are any choices in the response\n",
        "if completion.choices:\n",
        "    # Extract the message content\n",
        "    message_content = completion.choices[0].message.content\n",
        "    # Output the AI model's response\n",
        "    print(f\"AI Model's response: {message_content}\")\n",
        "else:\n",
        "    # If there are no choices, output an error message\n",
        "    print(\"Error: The AI model did not return any response.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEuc_xB-JQ9H",
        "outputId": "80ac78c4-e934-45a3-f2dc-fc53c39be17d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Model's response: 根据提供的文字内容进行分析，可以得出以下结论：\n",
            "\n",
            "1. 文字中提到了补差权益的具体信息，包括补差3900元可以获得48节直播课时、4次请假机会、2节互动自学课、1次休学机会、30次周分享机会等。因此，文字中包含了<补差权益>的关键信息点。\n",
            "\n",
            "2. 文字中提到了补差权益的有效期限，即补差的权益即将到期。因此，文字中包含了<权益到期>的关键信息点。\n",
            "\n",
            "综上所述，文字中包含了<补差权益>和<权益到期>的两个信息点，因此输出为2。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import codecs\n",
        "from openai import AzureOpenAI\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"a4dd22698d4b4b25bbf323ee7bcec472\"\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://tp-east-us.openai.azure.com/\"\n",
        "\n",
        "# Load JSON dataset\n",
        "data_path = \"/content/drive/MyDrive/Colab Notebooks/huohua_data_0308/CategoryOnly/train_sop905_0207.jsonl\"\n",
        "with codecs.open(data_path, encoding='utf-8-sig') as f:\n",
        "    json_dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# Set Azure OpenAI client\n",
        "client35 = AzureOpenAI(\n",
        "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
        "    api_version=\"2023-12-01-preview\",\n",
        "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        ")\n",
        "\n",
        "# Loop through each item in json_dataset and send to AzureOpenAI\n",
        "for index, data in enumerate(json_dataset):\n",
        "    messages = data[\"messages\"]\n",
        "\n",
        "    # Create the completion\n",
        "    completion = client35.chat.completions.create(\n",
        "        model=\"AIInnov-Jinse-GPT35-turbo-2\",\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    # Check if there are any choices in the response\n",
        "    if completion.choices:\n",
        "        # Extract the message content\n",
        "        message_content = completion.choices[0].message.content\n",
        "        # Output the AI model's response\n",
        "        print(f\"AI Model's response for json_dataset[{index}]: {message_content}\")\n",
        "    else:\n",
        "        # If there are no choices, output an error message\n",
        "        print(f\"Error: The AI model did not return any response for json_dataset[{index}].\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9-1LDuAcWUIy",
        "outputId": "0d78e896-892a-4198-e750-cf52042796c6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Model's response for json_dataset[0]: \n",
            "AI Model's response for json_dataset[1]: 根据提供的文字内容，可以确定该内容包含两个关键信息点：补差权益和已经补差。因此，输出结果为2。\n",
            "AI Model's response for json_dataset[2]: 根据提供的文字内容，可以得出以下结论：\n",
            "- 文字中没有提及补差权益的具体信息，也没有提及权益到期的时间和付款截止日期等信息。\n",
            "- 文字中包含关于已经补差的信息，即提到了补差的最后一天。\n",
            "基于以上分析，输出结果为2。\n",
            "AI Model's response for json_dataset[3]: 根据给定的文字内容，可以找到以下关键信息点：\n",
            "\n",
            "1. 补差权益：文字中提到补差需要支付4080元，以及课时和火花币已经到账。\n",
            "2. 权益到期：文字中提到补差权益今天晚上到期，只有9个小时了。\n",
            "\n",
            "因此，根据要求输出2。\n",
            "AI Model's response for json_dataset[4]: 根据提供的文字内容，我们可以找到以下信息点：\n",
            "\n",
            "1. 补差权益：文字中提到补差【2000】块钱的价格优惠。\n",
            "2. 权益到期：文字中提到补差权益即将到期，今天是最后一天。\n",
            "3. 已经补差：文字中提到课时已经到账了。\n",
            "\n",
            "根据以上信息点，该文字内容中包含<补差权益>和<权益到期>的信息，所以输出结果为2。\n",
            "AI Model's response for json_dataset[5]: 根据所提供的文字内容，可以判断该内容中包含了关于课程升级补差的关键信息点：<已经补差>和<权益到期>。因此，输出结果为2。\n",
            "AI Model's response for json_dataset[6]: 文字中包含了关于补差权益和权益到期的信息，因此输出2。\n",
            "AI Model's response for json_dataset[7]: 2\n",
            "AI Model's response for json_dataset[8]: \n",
            "AI Model's response for json_dataset[9]: \n",
            "AI Model's response for json_dataset[10]: 本文中包含了关于补差权益和权益到期的信息。补差权益包括补差3900元可以获得48节直播课时、4次请假机会、2节互动自学课、1次休学机会、30次周分享机会等。同时提到补差权益今晚就要过期了。因此，根据文字内容的分类，输出结果为2。\n",
            "AI Model's response for json_dataset[11]: 该文字中包含了关于课程升级补差的关键信息点：<补差权益>和<权益到期>。因此输出为2。\n",
            "AI Model's response for json_dataset[12]: 根据文字内容分析，该段文字中包含了关于已经补差的信息，因此输出结果为2。\n",
            "AI Model's response for json_dataset[13]: 本文包含了关于补差权益和权益到期的两个关键信息点。\n",
            "\n",
            "1. 关于补差权益：文字中提到补差金额为3900元，补差后可以获得48节直播课时，4次请假机会，2节互动自学课时，1次休学机会，30次周分享机会。\n",
            "2. 关于权益到期：文字中提到补差权益今天到期，以及补差权益今晚就到期了。\n",
            "\n",
            "因此，输出结果为2。\n",
            "AI Model's response for json_dataset[14]: \n",
            "AI Model's response for json_dataset[15]: 我检测到该内容中包含了<补差权益>和<权益到期>的关键信息点，因此输出结果为2。\n",
            "AI Model's response for json_dataset[16]: 根据提供的文字内容，可以得出以下结论：\n",
            "\n",
            "1. 文字中提到补差今天到期了，说明补差权益即将失效，属于<权益到期>的信息点。\n",
            "2. 文字中提到补差优惠很划算，家长参加了补差，相当于便宜了2000块钱，属于<补差权益>的信息点。\n",
            "3. 文字中没有提到已经补差的信息。\n",
            "\n",
            "因此，文字中包含<补差权益>和<权益到期>的信息，输出为2。\n",
            "AI Model's response for json_dataset[17]: \n",
            "AI Model's response for json_dataset[18]: 根据用户提供的文字内容，可以得出以下结论：\n",
            "\n",
            "1. 文字中提到了“补差权益今天就要到期”，说明包含了关于权益到期的信息。\n",
            "2. 文字中提到了“课时也到账了”，说明包含了关于已经补差的信息。\n",
            "\n",
            "综上所述，文字中包含了补差权益和权益到期的信息，输出结果为2。\n",
            "AI Model's response for json_dataset[19]: 根据提供的文字内容分析，这段文字并未包含关于课程升级补差的关键信息点，因此输出结果为0。\n",
            "AI Model's response for json_dataset[20]: 根据提供的文字内容，可以判断出其中包含了<补差权益>和<权益到期>的两个信息点。\n",
            "\n",
            "1. 从句子\"补差从没有人说过还有比这个更低不管咱们享受与否，有且仅有一次，后面都是5980元正价续费48课时包了\"中可以得知补差的金额优惠是3900元，购买内容包括48节直播课时、4次请假机会、2节互动自学课、1次休学机会和30次周分享机会。\n",
            "\n",
            "2. 从句子\"十分钟有效期\"可以推断出补差优惠的二维码有效期为10分钟。\n",
            "\n",
            "因此，根据要求输出结果为2。\n",
            "AI Model's response for json_dataset[21]: \n",
            "AI Model's response for json_dataset[22]: 2\n",
            "AI Model's response for json_dataset[23]: 这段文字中包含一些关于补差的信息：\n",
            "1. 文字中提到补差【3900】元就可以获得【48】节直播课时，【4】次请假机会，【2】节互动自学课，【1】次休学机会，【30】次周分享机会（可兑换6节直播课课时）等含义相近的关于价格优惠和购买内容的具体信息。\n",
            "2. 文字中提到补差赠送了一次抽奖机会，表示已经补差成功。\n",
            "\n",
            "因此，根据提示，输出结果为2。\n",
            "AI Model's response for json_dataset[24]: 根据提供的文字内容进行分析，可以得出以下结论：\n",
            "- 文字中包含了关于补差权益的信息，如“补差课包是3900，买48课包的，也会额外赠送6节朋友圈打卡”，提到了补差价格和购买内容。\n",
            "- 文字中没有明确提到补差优惠的有效期限或付款截止日期，所以不能判断是否包含权益到期的信息。\n",
            "- 文字中没有明确提到已经补差的信息，所以不能判断是否包含已经补差的信息。\n",
            "\n",
            "根据上述分析，文字中只包含<补差权益>的信息，不存在其他信息点。因此输出结果为1。\n",
            "AI Model's response for json_dataset[25]: 根据给出的文字内容，可以得出以下结论：\n",
            "\n",
            "1. 文字中包含关于补差权益和权益到期的信息。\n",
            "2. 第一段提到补差期最后一天就到期了，并且描述了补差权益的具体内容，如优惠价格和课时数量等。\n",
            "3. 第一段还提到了如果错过补差期后续课程的价格将会增加，暗示了权益即将失效。\n",
            "4. 第三段中提到课时已经到账，表示用户已经完成了补差支付。\n",
            "5. 最后一段中提到宝贝目前的基础较薄弱，需要学习更多的知识点，暗示了用户会继续学习课程。\n",
            "\n",
            "综上所述，文字中包含补差权益和权益到期的信息，同时也包含已经补差的信息，因此输出结果为2。\n",
            "AI Model's response for json_dataset[26]: 分析：\n",
            "根据提供的文字内容，可以得出以下判断：\n",
            "1. 文字中包含了关键信息点“已经补差”，如支付完成后跟我说一下，抽奖链接等。\n",
            "2. 文字中没有包含关键信息点“补差权益”和“权益到期”。\n",
            "\n",
            "根据要求输出结果，我们得出最终结果是：2。\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 4096 tokens. However, your messages resulted in 6844 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-893b6a542508>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Create the completion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     completion = client35.chat.completions.create(\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"AIInnov-Jinse-GPT35-turbo-2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 667\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    668\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         )\n\u001b[0;32m-> 1208\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 897\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    898\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 4096 tokens. However, your messages resulted in 6844 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c2xVp3THWwGa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}